import whisper
import os
import tempfile
from typing import Optional

import librosa
import soundfile as sf
try:
    import torch  # type: ignore
    _torch_available = True
except Exception:
    _torch_available = False

def transcribe_audio(file_path):
    """
    ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
    """
    try:
        print(f"ğŸµ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± Ø§Ø² ÙØ§ÛŒÙ„: {os.path.basename(file_path)}")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒØŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ±
        model_name = os.getenv("WHISPER_MODEL", "medium")
        print(f"ğŸ”„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Whisper {model_name}...")
        
        try:
            model = whisper.load_model(model_name)
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {model_name}: {e}")
            fallback_model = os.getenv("WHISPER_FALLBACK_MODEL", "medium")
            print(f"ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø§ Ù…Ø¯Ù„ {fallback_model}...")
            model = whisper.load_model(fallback_model)
            model_name = fallback_model

        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª: Ù…ÙˆÙ†Ùˆ Ùˆ 16kHz Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
        preprocess_enabled = os.getenv("WHISPER_PREPROCESS", "1") == "1"
        input_path = file_path
        tmp_path: Optional[str] = None
        if preprocess_enabled:
            try:
                audio, sr = librosa.load(file_path, sr=16000, mono=True)
                fd, tmp_path = tempfile.mkstemp(suffix=".wav")
                os.close(fd)
                sf.write(tmp_path, audio, 16000)
                input_path = tmp_path
                print("ğŸ§¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯ (mono, 16k)")
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª: {e}. Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ")

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„â€ŒÙ¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
        use_fp16 = _torch_available and torch.cuda.is_available()
        beam_size = int(os.getenv("WHISPER_BEAM_SIZE", "5"))
        best_of = int(os.getenv("WHISPER_BEST_OF", "5"))
        condition_prev = os.getenv("WHISPER_CONDITION_ON_PREVIOUS", "1") == "1"

        result = model.transcribe(
            input_path,
            language=os.getenv("WHISPER_LANGUAGE", "fa"),
            task="transcribe",
            fp16=use_fp16,
            verbose=True,
            temperature=0.0,
            compression_ratio_threshold=float(os.getenv("WHISPER_COMPRESSION_RATIO", "2.4")),
            logprob_threshold=float(os.getenv("WHISPER_LOGPROB_THRESHOLD", "-1.0")),
            no_speech_threshold=float(os.getenv("WHISPER_NO_SPEECH_THRESHOLD", "0.6")),
            condition_on_previous_text=condition_prev,
            initial_prompt=os.getenv("WHISPER_INITIAL_PROMPT", "Ø§ÛŒÙ† ÛŒÚ© Ù…Ú©Ø§Ù„Ù…Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª"),
            beam_size=beam_size,
            best_of=best_of
        )
        
        transcript = result["text"].strip()
        confidence = result.get("avg_logprob", 0)
        
        print(f"âœ… Ù…ØªÙ† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡: {transcript}")
        print(f"ğŸ“Š Ø§Ø·Ù…ÛŒÙ†Ø§Ù†: {confidence:.2f}")
        
        # Ø§Ú¯Ø± Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù… Ø§Ø³Øª Ùˆ Ù…Ø¯Ù„ base/medium Ù†ÛŒØ³ØªØŒ Ø¨Ø§ Ù…Ø¯Ù„ large ØªÙ„Ø§Ø´ Ú©Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        try_large = os.getenv("WHISPER_TRY_LARGE_ON_LOW_CONF", "1") == "1"
        if try_large and confidence < -1.0 and model_name != "large":
            print("âš ï¸ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ù…ØŒ ØªÙ„Ø§Ø´ Ø¨Ø§ Ù…Ø¯Ù„ large...")
            try:
                large_model = whisper.load_model("large")
                large_result = large_model.transcribe(
                    input_path,
                    language=os.getenv("WHISPER_LANGUAGE", "fa"),
                    temperature=0.0,
                    verbose=False,
                    beam_size=beam_size,
                    best_of=best_of
                )
                large_transcript = large_result["text"].strip()
                large_confidence = large_result.get("avg_logprob", 0)
                
                if large_confidence > confidence:
                    print(f"âœ… Ù…Ø¯Ù„ large Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯: {large_transcript}")
                    return large_transcript
                else:
                    print(f"âš ï¸ Ù…Ø¯Ù„ large Ø¨Ù‡ØªØ± Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†ØªÛŒØ¬Ù‡ Ù‚Ø¨Ù„ÛŒ")
                    return transcript
                    
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø¯Ù„ large: {e}")
                return transcript
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
        try:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass

        return transcript
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±: {e}")
        # fallback Ø¨Ù‡ Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©â€ŒØªØ±
        try:
            print("ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø§ Ù…Ø¯Ù„ base...")
            model = whisper.load_model("base")
            result = model.transcribe(file_path, language="fa")
            return result["text"].strip()
        except Exception as e2:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø¯Ù„ fallback: {e2}")
            return "Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø±"
